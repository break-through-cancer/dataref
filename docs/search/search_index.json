{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction The aim of this document is to serve as the definitive reference for data handling in Break Through Cancer (BTC), establishing the consensus rubric under which data are identified, categorized, generated, aggregated, accessed, and governed. This includes metadata capture during patient enrollment and sample acquisition, standards and processes for molecular assay data generation and pipelines, data flow diagrams providing simplified views, as well as a FAQ for common questions. In concert with BTC Disease TeamLabs, these norms are being codified by the Data Science TeamLab (DST) as a key element of the data science proposal The system and infrastructure which serves as the convergence point for data science activity within BTC is code-named DASH , short for DA ta S cience H ub; and is informed by numerous standards, including F.A.I.R. data practice and NIH guidelines , and draws heavily from lessons learned and software developed in earlier and sister projects, including HTAN , GDC , and TCGA Data Life Cycle The general flow of data within BTC is as follows: It is helpful to distinguish that the BTC data ecosystem has 2 layers of context: the first encompasses institutional and especially clinical trial activity, the other encompasses data activity vis-a-vis DASH proper. Layer 1 : Institutions conduct clinical trials and collect sample data in their own systems, per their current practice. This originating data will have trial-specific IDs & categorizations relevant in the context of that trial or institution. And is where PHI is maintained; such PHI will be removed prior to ingest to BTC. Layer 2: BTC will assign BTC-specific IDs when data are ingested into DASH. The BTC nomenclature and IDs supplement those of the institution and trial, rather than replace. The mapping between BTC and trial IDs will be retained internally by BTC, and is important provenance to prevent orphaning (see below). Note that DASH is still in the early stage of development and this data lifecycle is not fully operational. Reflecting the patient-centered BTC sense of urgency, to date more activity has been leftward, working with disease teams to identify how clinical and research needs translate into SOPs, staging and cloud-based data infrastructure so that data may flow . However, that work directly supports pipelined analyses and exploratory visualization capabilites moving rightward, and we are committed to an iterative, agile-inspired process of frequent data and software releases to further BTC goals. Submitting and Tracking Please contact the DASH team when your TeamLab is ready to add data to DASH. We'll be happy to guide the process and help decide which staging method is appropriate, as well as perform initial data validation; this may include verification with the TeamLab (and/or PI, etc) that the data are indeed eligible and ready for sharing, initial PHI assessment, verifying that trial-specific IDs (when applicable) have been entered into the data submission tracker and associated with corresponding BTC identifiers, and that sufficient metadata are captured. In addition to surfacing the inventory of data being readied for inclusion in DASH, the tracker will also indicate through what phase each dataset has progressed rightward during its life cycle. While the data SOP remains under development and not fully operational, this process may take a week or more after initial submission; but the tracker will reflect when data are accessible to the given teamlab via dashboards and available for download through the data browser , and eventually explorable in familiar tools like cBioPortal , cellxgene , or Minerva as appropriate. Sharing and Orphaning It is important to note that Submitting data to DASH does not confer access to other TeamLabs in BTC: until the TeamLab collectively decides to provide such data to other TeamLabs, access is limited to members of the submitting TeamLab Clinical lab manuals and SOPs outline steps for registering trial-specific subject and sample IDs in DASH, to prevent orphaning : when the provenance of physical material or data cannot be established because it's been shipped from one institute to another either (a) without trial-specific IDs or (b) recording those trial-specific IDs in DASH and assigning corresponding BTC IDs Staging Area The staging area is an abstraction that serves as the entryway into DASH: staging of data generated within a disease TeamLab signifies that it is ready to be freely shared amongst other members of the team. This can be done in one of two ways: first , to simplify and expedite immediate data sharing by using existing drag-n-drop infrastructure wherever possible, small images, figures and documents (including pre-clinical) can be deposited directly to the Data folder within the team's respective SharePoint area; second , larger data, such as the molecular results of DNA or scRNA sequencing assays, should be uploaded to data lake cloud storage. To aid clarity and automation, we suggest organizing files into consistent subfolder names according to their data type, along the lines of Data Lake After staging, data will be migrated to the data lake: a semi-organized area of cloud storage containing the files \"as deposited,\" meaning no additional processing, subdivision or interpretation has been performed upon the uploaded files--other than stratification by TeamLab, BTC identifiers, and assay data type. Cloud storage in the data lake (i.e. buckets ) will be accessible for systematic analysis in downstream pipelines and tools where the medium of exchange is coarse grained--at the level of entire files rather than portions of content within the files. Finer grained access, such as the ability to query individual rows, columns or data elements within the files (e.g. the expression level of a single gene from a given sample) is typically not performed directly from a data lake but rather via downstream database queries, dashboards or visual-exploratory interfaces (further right in the data life cycle). Warehouse The purpose of the data warehouse is to provide additional validation and cataloging of data within the lake, distilled into high-level dashboards and portals queryable by way of a simple, point-click interface. Due to the ease with which they provide comprehensive summary views of data-intensive science, such portals can be an important means of initial engagement with research data. The BTC warehouse is under construction, but many other examples abound, including the GDC and HTAN ; here we show an excerpt from the GTEx portal Analysis and Pipelines The DST is working with disease teams to compile data and analysis needs cutting across BTC efforts and For each data type in this list the DST will provide pipelines and/or tools for each data level : from Levels 1 and 2 primary data generation (L1, L2) through subsequent L3 and L4 analyses. This work is still early but very active, and a number of analysis pipelines are already available from Terra , MDAnderson, NextFlow nf-core and the MIT BioMicroCenter . Please contact the DASH team for more information if you are unsure of how to proceed. BTC Identifier Scheme Subject and sample (biospecimen) identifiers will be attached to BTC data as follows: Here \u201cbiospecimen\u201d is preferred over \u201csample\u201d for generality and to capture that samples can be subdivided into multiple portions. The association between data file and biospecimen is maintained as metadata within the DASH database, not within the file identifier itself. A hypothetical data tree for the first subject (patient) of the BTC glioblastoma multiforme trial might look like Here 6 needle biopsy cores (samples) were extracted; the first of which has been characterized in multiple assays, yielding 8 distinctly molecular data output files (i.e. one per data type). Each interventional timepoint in a longitudinal trial would yield a new sample (or samples) for that subject. Data Entities and Levels As data flow in BTC they are processed and transformed, passing through a series of \u201cdata levels.\u201d For each data context (e.g. clinical, imaging, sequencing, spatial) the constituent files of each data level may differ, but in all cases Level 1 represents raw or uncurated data (e.g. directly from an instrument) and each successive level represents a maturation of that data towards analysis and, eventually, publication and wider utilization. Unless explicitly stated otherwise, we propose BTC data infrastructure, processing and analysis adopt existing GDC + HTAN standards and nomenclature, including for clinical , biospecimen , sequencing and imaging data. For convenience, some of those standards will be excerpted below but we refer the reader back to the original material at the given links for an exhaustive treatment. Each disease project in BTC contributes data from 1 or more enrolled Subjects, who have donated Biospecimens. Data files are generated when biospecimens are assayed by an instrument/protocol or processed in downstream SOPs or analyses. Captured metadata enables tracing of any data file back to its source biospecimen. Level 1 raw data files are derived directly from the corresponding biospecimen, whereas processed level 2-4 data files are linked to lower level parent data files. Clinical Data Levels (Tiers) The BTC clinical data model consists of three tiers. Tier 1 aligns with Genomic Data Commons (GDC) guidelines for clinical data, while Tiers 2 and 3 are informed by the HTAN extensions to the GDC model . Clinical data in BTC are still evolving as we develop trial forms and SOPs across institutions. Only Tier1 is encompassed thus far, but no data will be ingested into DASH if it is not (a) fully de-identified and (b) accompanied by minimally viable metadata (biospecimen and/or clinical). Omic Data Levels In alignment with TCGA and the NCI Genomic Data Commons, BTC will categorize multi-omic data into four levels: Level Definition Examples 1 Raw data FASTQs, unaligned BAMs 2 Aligned primary data Aligned BAMs 3 Derived biomolecular data Gene expression matrix files, VCFs, etc 4 Sample level summary data t-SNE plot coordinates, etc These will apply to the multiple assay and sequencing modalities (omic data types) envisioned for BTC , including single-cell and single-nucleus RNA Seq (sc/snRNASeq), single-cell ATAC Seq, bulk RNAseq and Bulk DNAseq. We propose that BTC follow the latest GENCODE version for gene annotations, GENCODE Version 43. GENCODE is used for gene definitions by many consortia, including ENCODE, NCI Genomic Data Commons, Human Cell Atlas, and PCAWG (Pan-Cancer Analysis of Whole Genomes). Ensembl gene content is essentially identical to that of GENCODE (FAQ) and interconversion is possible. We further propose BTC adopt GENCODE 43 Gene Transfer Format (GTF) basic gene annotation file (GENCODE 43 GTF) and filtered files (GENCODE 43 GTF with genes only; GENCODE 43 GTF with genes only and retaining only chromosome X copy of pseudoautosomal region) for gene annotation. BTC may also include external data generated with other gene models, as the process of implementing the standard is ongoing. Within BTC metadata files, we propose the reference genome in use be specified in columns/attributes named \u201cGenomic Reference\u201d and \u201cGenomic Reference URL\u201d. External Data Data generated through efforts funded by other organizations is referred to as \u201cexternal data\u201d. BTC investigators are free (and encouraged) to utilize external data in BTC work, which will typically play out in one of two ways: Ad-hoc: in which the investigator or their staff downloads external data to local institutional resources (e.g. on-prem compute or cloud) and references in their local BTC analyses; here the investigator and/or staff initiates & performs the collecting, aggregating and storing of external data on their institutional systems DASH: investigator requests that BTC make external easily accessible to other BTC collaborators via DASH; either in raw form directly from the data lake or more formally with BTC identifiers attached so that the identity and provenance are clear In the latter case of external data being assigned BTC IDs, a unique project abbreviation will be created to indicate that the dataset is from an external source, following the schema EXT_ . For example, if we were to load BRCA data from TCGA into DASH, one might use TCGA_BRCA as the project abbreviation, yielding identifiers for such data that begin with BTC-TCGA_BRCA- and so forth. Attaching BTC identifiers to external data may provide a number of advantages Enabling it to be seamlessly mingled with internal BTC data Then processed and analyzed at scale in downstream pipelines or analysis tools While making the external identity and provenance of the data clear And simplifying later bookkeeping and database tracing when assembling data for publication but should not be interpreted as a claim that BTC now \u201cowns\u201d or is attempting to \u201cre-brand\u201d those external data. Version 0.51","title":"  "},{"location":"#introduction","text":"The aim of this document is to serve as the definitive reference for data handling in Break Through Cancer (BTC), establishing the consensus rubric under which data are identified, categorized, generated, aggregated, accessed, and governed. This includes metadata capture during patient enrollment and sample acquisition, standards and processes for molecular assay data generation and pipelines, data flow diagrams providing simplified views, as well as a FAQ for common questions. In concert with BTC Disease TeamLabs, these norms are being codified by the Data Science TeamLab (DST) as a key element of the data science proposal The system and infrastructure which serves as the convergence point for data science activity within BTC is code-named DASH , short for DA ta S cience H ub; and is informed by numerous standards, including F.A.I.R. data practice and NIH guidelines , and draws heavily from lessons learned and software developed in earlier and sister projects, including HTAN , GDC , and TCGA","title":"Introduction"},{"location":"#data-life-cycle","text":"The general flow of data within BTC is as follows: It is helpful to distinguish that the BTC data ecosystem has 2 layers of context: the first encompasses institutional and especially clinical trial activity, the other encompasses data activity vis-a-vis DASH proper. Layer 1 : Institutions conduct clinical trials and collect sample data in their own systems, per their current practice. This originating data will have trial-specific IDs & categorizations relevant in the context of that trial or institution. And is where PHI is maintained; such PHI will be removed prior to ingest to BTC. Layer 2: BTC will assign BTC-specific IDs when data are ingested into DASH. The BTC nomenclature and IDs supplement those of the institution and trial, rather than replace. The mapping between BTC and trial IDs will be retained internally by BTC, and is important provenance to prevent orphaning (see below). Note that DASH is still in the early stage of development and this data lifecycle is not fully operational. Reflecting the patient-centered BTC sense of urgency, to date more activity has been leftward, working with disease teams to identify how clinical and research needs translate into SOPs, staging and cloud-based data infrastructure so that data may flow . However, that work directly supports pipelined analyses and exploratory visualization capabilites moving rightward, and we are committed to an iterative, agile-inspired process of frequent data and software releases to further BTC goals.","title":"Data Life Cycle"},{"location":"#submitting-and-tracking","text":"Please contact the DASH team when your TeamLab is ready to add data to DASH. We'll be happy to guide the process and help decide which staging method is appropriate, as well as perform initial data validation; this may include verification with the TeamLab (and/or PI, etc) that the data are indeed eligible and ready for sharing, initial PHI assessment, verifying that trial-specific IDs (when applicable) have been entered into the data submission tracker and associated with corresponding BTC identifiers, and that sufficient metadata are captured. In addition to surfacing the inventory of data being readied for inclusion in DASH, the tracker will also indicate through what phase each dataset has progressed rightward during its life cycle. While the data SOP remains under development and not fully operational, this process may take a week or more after initial submission; but the tracker will reflect when data are accessible to the given teamlab via dashboards and available for download through the data browser , and eventually explorable in familiar tools like cBioPortal , cellxgene , or Minerva as appropriate.","title":"Submitting and Tracking"},{"location":"#sharing-and-orphaning","text":"It is important to note that Submitting data to DASH does not confer access to other TeamLabs in BTC: until the TeamLab collectively decides to provide such data to other TeamLabs, access is limited to members of the submitting TeamLab Clinical lab manuals and SOPs outline steps for registering trial-specific subject and sample IDs in DASH, to prevent orphaning : when the provenance of physical material or data cannot be established because it's been shipped from one institute to another either (a) without trial-specific IDs or (b) recording those trial-specific IDs in DASH and assigning corresponding BTC IDs","title":"Sharing and Orphaning"},{"location":"#staging-area","text":"The staging area is an abstraction that serves as the entryway into DASH: staging of data generated within a disease TeamLab signifies that it is ready to be freely shared amongst other members of the team. This can be done in one of two ways: first , to simplify and expedite immediate data sharing by using existing drag-n-drop infrastructure wherever possible, small images, figures and documents (including pre-clinical) can be deposited directly to the Data folder within the team's respective SharePoint area; second , larger data, such as the molecular results of DNA or scRNA sequencing assays, should be uploaded to data lake cloud storage. To aid clarity and automation, we suggest organizing files into consistent subfolder names according to their data type, along the lines of","title":"Staging Area"},{"location":"#data-lake","text":"After staging, data will be migrated to the data lake: a semi-organized area of cloud storage containing the files \"as deposited,\" meaning no additional processing, subdivision or interpretation has been performed upon the uploaded files--other than stratification by TeamLab, BTC identifiers, and assay data type. Cloud storage in the data lake (i.e. buckets ) will be accessible for systematic analysis in downstream pipelines and tools where the medium of exchange is coarse grained--at the level of entire files rather than portions of content within the files. Finer grained access, such as the ability to query individual rows, columns or data elements within the files (e.g. the expression level of a single gene from a given sample) is typically not performed directly from a data lake but rather via downstream database queries, dashboards or visual-exploratory interfaces (further right in the data life cycle).","title":"Data Lake"},{"location":"#warehouse","text":"The purpose of the data warehouse is to provide additional validation and cataloging of data within the lake, distilled into high-level dashboards and portals queryable by way of a simple, point-click interface. Due to the ease with which they provide comprehensive summary views of data-intensive science, such portals can be an important means of initial engagement with research data. The BTC warehouse is under construction, but many other examples abound, including the GDC and HTAN ; here we show an excerpt from the GTEx portal","title":"Warehouse"},{"location":"#analysis-and-pipelines","text":"The DST is working with disease teams to compile data and analysis needs cutting across BTC efforts and For each data type in this list the DST will provide pipelines and/or tools for each data level : from Levels 1 and 2 primary data generation (L1, L2) through subsequent L3 and L4 analyses. This work is still early but very active, and a number of analysis pipelines are already available from Terra , MDAnderson, NextFlow nf-core and the MIT BioMicroCenter . Please contact the DASH team for more information if you are unsure of how to proceed.","title":"Analysis and Pipelines"},{"location":"#btc-identifier-scheme","text":"Subject and sample (biospecimen) identifiers will be attached to BTC data as follows: Here \u201cbiospecimen\u201d is preferred over \u201csample\u201d for generality and to capture that samples can be subdivided into multiple portions. The association between data file and biospecimen is maintained as metadata within the DASH database, not within the file identifier itself. A hypothetical data tree for the first subject (patient) of the BTC glioblastoma multiforme trial might look like Here 6 needle biopsy cores (samples) were extracted; the first of which has been characterized in multiple assays, yielding 8 distinctly molecular data output files (i.e. one per data type). Each interventional timepoint in a longitudinal trial would yield a new sample (or samples) for that subject.","title":"BTC Identifier Scheme"},{"location":"#data-entities-and-levels","text":"As data flow in BTC they are processed and transformed, passing through a series of \u201cdata levels.\u201d For each data context (e.g. clinical, imaging, sequencing, spatial) the constituent files of each data level may differ, but in all cases Level 1 represents raw or uncurated data (e.g. directly from an instrument) and each successive level represents a maturation of that data towards analysis and, eventually, publication and wider utilization. Unless explicitly stated otherwise, we propose BTC data infrastructure, processing and analysis adopt existing GDC + HTAN standards and nomenclature, including for clinical , biospecimen , sequencing and imaging data. For convenience, some of those standards will be excerpted below but we refer the reader back to the original material at the given links for an exhaustive treatment. Each disease project in BTC contributes data from 1 or more enrolled Subjects, who have donated Biospecimens. Data files are generated when biospecimens are assayed by an instrument/protocol or processed in downstream SOPs or analyses. Captured metadata enables tracing of any data file back to its source biospecimen. Level 1 raw data files are derived directly from the corresponding biospecimen, whereas processed level 2-4 data files are linked to lower level parent data files.","title":"Data Entities and Levels"},{"location":"#clinical-data-levels-tiers","text":"The BTC clinical data model consists of three tiers. Tier 1 aligns with Genomic Data Commons (GDC) guidelines for clinical data, while Tiers 2 and 3 are informed by the HTAN extensions to the GDC model . Clinical data in BTC are still evolving as we develop trial forms and SOPs across institutions. Only Tier1 is encompassed thus far, but no data will be ingested into DASH if it is not (a) fully de-identified and (b) accompanied by minimally viable metadata (biospecimen and/or clinical).","title":"Clinical Data Levels (Tiers)"},{"location":"#omic-data-levels","text":"In alignment with TCGA and the NCI Genomic Data Commons, BTC will categorize multi-omic data into four levels: Level Definition Examples 1 Raw data FASTQs, unaligned BAMs 2 Aligned primary data Aligned BAMs 3 Derived biomolecular data Gene expression matrix files, VCFs, etc 4 Sample level summary data t-SNE plot coordinates, etc These will apply to the multiple assay and sequencing modalities (omic data types) envisioned for BTC , including single-cell and single-nucleus RNA Seq (sc/snRNASeq), single-cell ATAC Seq, bulk RNAseq and Bulk DNAseq. We propose that BTC follow the latest GENCODE version for gene annotations, GENCODE Version 43. GENCODE is used for gene definitions by many consortia, including ENCODE, NCI Genomic Data Commons, Human Cell Atlas, and PCAWG (Pan-Cancer Analysis of Whole Genomes). Ensembl gene content is essentially identical to that of GENCODE (FAQ) and interconversion is possible. We further propose BTC adopt GENCODE 43 Gene Transfer Format (GTF) basic gene annotation file (GENCODE 43 GTF) and filtered files (GENCODE 43 GTF with genes only; GENCODE 43 GTF with genes only and retaining only chromosome X copy of pseudoautosomal region) for gene annotation. BTC may also include external data generated with other gene models, as the process of implementing the standard is ongoing. Within BTC metadata files, we propose the reference genome in use be specified in columns/attributes named \u201cGenomic Reference\u201d and \u201cGenomic Reference URL\u201d.","title":"Omic Data Levels"},{"location":"#external-data","text":"Data generated through efforts funded by other organizations is referred to as \u201cexternal data\u201d. BTC investigators are free (and encouraged) to utilize external data in BTC work, which will typically play out in one of two ways: Ad-hoc: in which the investigator or their staff downloads external data to local institutional resources (e.g. on-prem compute or cloud) and references in their local BTC analyses; here the investigator and/or staff initiates & performs the collecting, aggregating and storing of external data on their institutional systems DASH: investigator requests that BTC make external easily accessible to other BTC collaborators via DASH; either in raw form directly from the data lake or more formally with BTC identifiers attached so that the identity and provenance are clear In the latter case of external data being assigned BTC IDs, a unique project abbreviation will be created to indicate that the dataset is from an external source, following the schema EXT_ . For example, if we were to load BRCA data from TCGA into DASH, one might use TCGA_BRCA as the project abbreviation, yielding identifiers for such data that begin with BTC-TCGA_BRCA- and so forth. Attaching BTC identifiers to external data may provide a number of advantages Enabling it to be seamlessly mingled with internal BTC data Then processed and analyzed at scale in downstream pipelines or analysis tools While making the external identity and provenance of the data clear And simplifying later bookkeeping and database tracing when assembling data for publication but should not be interpreted as a claim that BTC now \u201cowns\u201d or is attempting to \u201cre-brand\u201d those external data.","title":"External Data"},{"location":"#version-051","text":"","title":"Version 0.51"},{"location":"about/","text":".navbar { display: none; } BTC Data Science in Brief The primary goal of the Data Science TeamLab (DST) within Break Through Cancer (BTC) is to unify BTC investigators around data and computation, into a collaborative network that will enhance and accelerate all disease research and discovery objectives. By providing essential data infrastructure and advanced computational cancer biology, in support of the scientific objectives and the experimentalist and clinical team members of BTC projects, we aim to increase the efficiency, durability and robustness of all BTC research. By making a dedicated investment in the development of new computational methods and software, we aspire to advance biological questions and themes cutting across all BTC disease teams, including cancer evolution & cellular plasticity spatial determinants of microenvironments and tumor cell immune cell interactions Moreover, in the collaborative spirit of BTC, our network-wide model also provides a unique social component dedicated to mentoring the next generation of outstanding investigators. The data science initiative within BTC thus constitutes a unique and transformative multi-institutional initiative that will be continuously responsive to the scientific needs of diease projects and investigators practice rigorous and secure data engineering, informed by F.A.I.R. principles accelerate advances in computational methods required to achieve BTC scientific aims create robust, benchmarked and hardened software for scaled, systematic deployment maximize the impact of each BTC project by enabling and accelerating pan-cancer analysis; create a collaborative network of data science investigators and trainees to power these objectives For more details, please see the full data science proposal here. How We'll Work Together As noted in these slides excerpted from our kickoff , the Data Science TeamLab sees itself as Facilitators of disease TeamLab science Innovators of high-resolution, multi-modal analytic methods Weavers of infrastructural fabric linking these together thereby breaking through traditional research silos to foster the widest, deepest, and fastest cross-institutional engagement of BTC expertise, within a secure and well-governed platform. We hope to achieve this through dynamic and transparent collaboration with each disease team, to learn where we may help each other, how, and at what cadence. This may often take the form of data scientists helping to answer a myriad of scientific questions posed by the disease group, co-analyzing data, and using the resulting insights to drive the development of new computational methods. At other moments it might be engineers co-developing a database model, or forms for gathering clinical parameters, or assisting with data upload to cloud storage and automated analysis. At still other points, it might involve co-developing pipeline methods or exploratory visualizations and dashboarding tools, or guiding others in their use. The work will play out in a variety of ways, but this is just the beginning of a transformative experiment in multi-institutional, collaborative science; we know our collective efforts will lead to opportunities and challenges not yet envisioned, but look forward to working with disease teams to identify and prioritize areas of greatest unmet scientific, algorithmic, or technical need and cultivating solutions that accelerate our science. Primary Contacts We are fortunate to have the talents of Benjamin Greenbaum and Kadir Akdemir to coordinate our monthly meeting series and related activity, so for general matters please reach out to them. For administrative questions, please contact Ineke Ceder, and for our proposal aims the primary contacts are as follows: Aim 1 - Best Practices Pipelines: Rameen Beroukhim , Linghua Wang Aim 2 \u2013 Data Infrastructure and Governance: Michael Noble Aim 3 - Algorithms and Methods: Sohrab Shah Aim 4 - Evaluate/Benchmark/Harden Software: Andrew McPherson , Charlie Whitaker Aim 5 - Pan-cancer Analysis: Linghua Wang , Rameen Beroukhim Aim 6 - BTC Data Science Network: Kadir Akdemir , Elana Fertig These subgroups are still in the early stages but are gathering on a bi-weekly to monthly cadence and have an active presence across disease TeamLabs. For reference, the list of Data Science Team members and email addresses is here . Data Infrastructure BTC is unique in the foresight and boldness of its commitment to placing collaborative data science at the center of a diverse research portfolio. But realizing that long-term vision will not be easy or quick, so it is worth emphasizing that early versions of infrastructure and tools will be enabling but minimalist; and, like all early-stage work, will look and feel more like exposed foundation and scaffolding. With each successive iteration, however, we will identify what critical needs remain unmet and what features to keep or eliminate, while emphasizing clarity, transparency, and simplicity. Here is a summary of recent thought progress on infrastructure and how that informs planning at several scales. Regular updates will be given to all of BTC as these efforts mature, and detailed plans will be circulated as they emerge. Early, Medium, and Long-Term Planning In the nearest term and as a stopgap measure while hiring staff, finalizing partner SOWs, and gradually assembling infrastructure, we\u2019ve constructed a prototype data lake using the Google Cloud Platform (GCP). While just the first step towards our long-term vision, this reflects our patient \u00fcber alles sense of urgency that to best support ongoing BTC science data must flow . The data lake will provide a central point of aggregation and access, and facilitate immediate sharing within TeamLabs because it does not require extensive build-out nor deep modeling, validation or annotation. We are working closely with clinical data coordinators and thought leaders across BTC to identify SOPs, data and annotations of greatest immediate impact, while prototyping lightweight mechanisms for submission of molecular and clinical data. As these data accumulate, we will also host a BTC-wide dashboard to foster transparency on accrual. In the medium term and drawing from substantial discussion and technical review with stakeholders and technology partners, we will further improve data flow through a pilot with Sage Bionetworks. Guided by a core philosophy of reuse rather than reinvent, using Synapse to validate ingestion will endow our data store with provenance and provable correctness; directly leverage the GCP data lake; stimulate rapid progress by reusing existing standards and open-source components developed in sister projects like HTAN; and reduce development time and cost. As that matures, we\u2019ll begin to assemble and operationalize a standard analysis workflow, which can either make direct use of the data lake or Synapse warehouse. Early versions of the analysis workflow will focus on tools already seeing wide use in the community (e.g. NextFlow Core ) and/or pre-installed in Terra ; but as new methods develop across BTC and beyond we\u2019ll seek to harden them for scale and incorporate into the standard analysis as well. For the long term, and given the complexity and rapid evolution of technology and science in cancer research, we recognize that no single platform addresses our entire slate of data collection, governance, and analysis needs. We will therefore continue to engage a wide range of technology partners\u2014including Verily/Google, Microsoft, Code Ocean, Syntropy, Flywheel, and Seven Bridges; towards defining proof-of-concept pilots to discern areas of mutual collaborative interest and potential philanthropic opportunity, while reaching for continuous improvement of the scientific and clinical capability, governance, reproducibility, scalability, security, and robustness of our system.","title":"  "},{"location":"about/#btc-data-science-in-brief","text":"The primary goal of the Data Science TeamLab (DST) within Break Through Cancer (BTC) is to unify BTC investigators around data and computation, into a collaborative network that will enhance and accelerate all disease research and discovery objectives. By providing essential data infrastructure and advanced computational cancer biology, in support of the scientific objectives and the experimentalist and clinical team members of BTC projects, we aim to increase the efficiency, durability and robustness of all BTC research. By making a dedicated investment in the development of new computational methods and software, we aspire to advance biological questions and themes cutting across all BTC disease teams, including cancer evolution & cellular plasticity spatial determinants of microenvironments and tumor cell immune cell interactions Moreover, in the collaborative spirit of BTC, our network-wide model also provides a unique social component dedicated to mentoring the next generation of outstanding investigators. The data science initiative within BTC thus constitutes a unique and transformative multi-institutional initiative that will be continuously responsive to the scientific needs of diease projects and investigators practice rigorous and secure data engineering, informed by F.A.I.R. principles accelerate advances in computational methods required to achieve BTC scientific aims create robust, benchmarked and hardened software for scaled, systematic deployment maximize the impact of each BTC project by enabling and accelerating pan-cancer analysis; create a collaborative network of data science investigators and trainees to power these objectives For more details, please see the full data science proposal here.","title":"BTC Data Science in Brief"},{"location":"about/#how-well-work-together","text":"As noted in these slides excerpted from our kickoff , the Data Science TeamLab sees itself as Facilitators of disease TeamLab science Innovators of high-resolution, multi-modal analytic methods Weavers of infrastructural fabric linking these together thereby breaking through traditional research silos to foster the widest, deepest, and fastest cross-institutional engagement of BTC expertise, within a secure and well-governed platform. We hope to achieve this through dynamic and transparent collaboration with each disease team, to learn where we may help each other, how, and at what cadence. This may often take the form of data scientists helping to answer a myriad of scientific questions posed by the disease group, co-analyzing data, and using the resulting insights to drive the development of new computational methods. At other moments it might be engineers co-developing a database model, or forms for gathering clinical parameters, or assisting with data upload to cloud storage and automated analysis. At still other points, it might involve co-developing pipeline methods or exploratory visualizations and dashboarding tools, or guiding others in their use. The work will play out in a variety of ways, but this is just the beginning of a transformative experiment in multi-institutional, collaborative science; we know our collective efforts will lead to opportunities and challenges not yet envisioned, but look forward to working with disease teams to identify and prioritize areas of greatest unmet scientific, algorithmic, or technical need and cultivating solutions that accelerate our science.","title":"How We'll Work Together"},{"location":"about/#primary-contacts","text":"We are fortunate to have the talents of Benjamin Greenbaum and Kadir Akdemir to coordinate our monthly meeting series and related activity, so for general matters please reach out to them. For administrative questions, please contact Ineke Ceder, and for our proposal aims the primary contacts are as follows: Aim 1 - Best Practices Pipelines: Rameen Beroukhim , Linghua Wang Aim 2 \u2013 Data Infrastructure and Governance: Michael Noble Aim 3 - Algorithms and Methods: Sohrab Shah Aim 4 - Evaluate/Benchmark/Harden Software: Andrew McPherson , Charlie Whitaker Aim 5 - Pan-cancer Analysis: Linghua Wang , Rameen Beroukhim Aim 6 - BTC Data Science Network: Kadir Akdemir , Elana Fertig These subgroups are still in the early stages but are gathering on a bi-weekly to monthly cadence and have an active presence across disease TeamLabs. For reference, the list of Data Science Team members and email addresses is here .","title":"Primary Contacts"},{"location":"about/#data-infrastructure","text":"BTC is unique in the foresight and boldness of its commitment to placing collaborative data science at the center of a diverse research portfolio. But realizing that long-term vision will not be easy or quick, so it is worth emphasizing that early versions of infrastructure and tools will be enabling but minimalist; and, like all early-stage work, will look and feel more like exposed foundation and scaffolding. With each successive iteration, however, we will identify what critical needs remain unmet and what features to keep or eliminate, while emphasizing clarity, transparency, and simplicity. Here is a summary of recent thought progress on infrastructure and how that informs planning at several scales. Regular updates will be given to all of BTC as these efforts mature, and detailed plans will be circulated as they emerge.","title":"Data Infrastructure"},{"location":"about/#early-medium-and-long-term-planning","text":"In the nearest term and as a stopgap measure while hiring staff, finalizing partner SOWs, and gradually assembling infrastructure, we\u2019ve constructed a prototype data lake using the Google Cloud Platform (GCP). While just the first step towards our long-term vision, this reflects our patient \u00fcber alles sense of urgency that to best support ongoing BTC science data must flow . The data lake will provide a central point of aggregation and access, and facilitate immediate sharing within TeamLabs because it does not require extensive build-out nor deep modeling, validation or annotation. We are working closely with clinical data coordinators and thought leaders across BTC to identify SOPs, data and annotations of greatest immediate impact, while prototyping lightweight mechanisms for submission of molecular and clinical data. As these data accumulate, we will also host a BTC-wide dashboard to foster transparency on accrual. In the medium term and drawing from substantial discussion and technical review with stakeholders and technology partners, we will further improve data flow through a pilot with Sage Bionetworks. Guided by a core philosophy of reuse rather than reinvent, using Synapse to validate ingestion will endow our data store with provenance and provable correctness; directly leverage the GCP data lake; stimulate rapid progress by reusing existing standards and open-source components developed in sister projects like HTAN; and reduce development time and cost. As that matures, we\u2019ll begin to assemble and operationalize a standard analysis workflow, which can either make direct use of the data lake or Synapse warehouse. Early versions of the analysis workflow will focus on tools already seeing wide use in the community (e.g. NextFlow Core ) and/or pre-installed in Terra ; but as new methods develop across BTC and beyond we\u2019ll seek to harden them for scale and incorporate into the standard analysis as well. For the long term, and given the complexity and rapid evolution of technology and science in cancer research, we recognize that no single platform addresses our entire slate of data collection, governance, and analysis needs. We will therefore continue to engage a wide range of technology partners\u2014including Verily/Google, Microsoft, Code Ocean, Syntropy, Flywheel, and Seven Bridges; towards defining proof-of-concept pilots to discern areas of mutual collaborative interest and potential philanthropic opportunity, while reaching for continuous improvement of the scientific and clinical capability, governance, reproducibility, scalability, security, and robustness of our system.","title":"Early, Medium, and Long-Term Planning"},{"location":"faq/","text":"BTC Data Science Frequently Asked Questions .bs-sidebar { display: none; } Who is responsible for data analysis within BTC disease team labs? There is fuzzy overlap in responsibility for analysis: it should be driven to the greatest extent possible by the disease teamlab, as reflected in their respective budgets, and supported by the Data Science TeamLab (DST) in whatever means necessary (whether computational analytics, data engineering, visualization, etc), but not wholly outsourced from the disease teamlab to the DST. Where that line is drawn for each scientific question or project will likely shift\u2014and may also depend upon trial timelines and staffing\u2014-probably in some cases the data scientists may need to pick up more of the analysis, in others less so. But the overall view, at least initially while BTC finds its operational footing vis-\u00e0-vis data science, is that the DST is not \u201cjust another core, service-oriented facility,\u201d but rather is a resource of expert collaborators who devise new methods (when needed), or assist with data gathering/engineering, or provide analytic insight and/or assist with interpretation. What pipelines will be available to assist with my analysis? This is discussed in the data reference . What assays (data types) will the pipelines operate upon? This is also discussed in the data reference . Ideally, what kind of biospecimen metadata will be collected? Sample acquisition method, e.g. autopsy, biopsy, fine needle aspirate, etc. Topography Code, indicating site within the body, e.g. based on ICD-O-3. Collection information e.g. time, duration of ischemia, temperature, etc. Processing of parent biospecimen information e.g. fresh, frozen, etc. Biospecimen and derivative clinical metadata I.e. Histologic Morphology Code, e.g. based on ICD-O-3. Coordinates for derivative biospecimen from their parent biospecimen. Processing of derivative biospecimen for downstream analysis e.g. dissociation, sectioning, analyte isolation This list is adapted from HTAN standards and is not intended to be mandatory constraints imposed by the DST upon clinical trial teams or disease team labs; we are in the process of defining the minimum viable set of clinical data elements (CDEs) for BTC projects. How do I submit data from my TeamLab into DASH? This is discussed in the data reference .","title":"FAQ"},{"location":"faq/#btc-data-science-frequently-asked-questions","text":".bs-sidebar { display: none; }","title":"BTC Data Science Frequently Asked Questions"},{"location":"faq/#_1","text":"Who is responsible for data analysis within BTC disease team labs? There is fuzzy overlap in responsibility for analysis: it should be driven to the greatest extent possible by the disease teamlab, as reflected in their respective budgets, and supported by the Data Science TeamLab (DST) in whatever means necessary (whether computational analytics, data engineering, visualization, etc), but not wholly outsourced from the disease teamlab to the DST. Where that line is drawn for each scientific question or project will likely shift\u2014and may also depend upon trial timelines and staffing\u2014-probably in some cases the data scientists may need to pick up more of the analysis, in others less so. But the overall view, at least initially while BTC finds its operational footing vis-\u00e0-vis data science, is that the DST is not \u201cjust another core, service-oriented facility,\u201d but rather is a resource of expert collaborators who devise new methods (when needed), or assist with data gathering/engineering, or provide analytic insight and/or assist with interpretation. What pipelines will be available to assist with my analysis? This is discussed in the data reference . What assays (data types) will the pipelines operate upon? This is also discussed in the data reference . Ideally, what kind of biospecimen metadata will be collected? Sample acquisition method, e.g. autopsy, biopsy, fine needle aspirate, etc. Topography Code, indicating site within the body, e.g. based on ICD-O-3. Collection information e.g. time, duration of ischemia, temperature, etc. Processing of parent biospecimen information e.g. fresh, frozen, etc. Biospecimen and derivative clinical metadata I.e. Histologic Morphology Code, e.g. based on ICD-O-3. Coordinates for derivative biospecimen from their parent biospecimen. Processing of derivative biospecimen for downstream analysis e.g. dissociation, sectioning, analyte isolation This list is adapted from HTAN standards and is not intended to be mandatory constraints imposed by the DST upon clinical trial teams or disease team labs; we are in the process of defining the minimum viable set of clinical data elements (CDEs) for BTC projects. How do I submit data from my TeamLab into DASH? This is discussed in the data reference .","title":""}]}