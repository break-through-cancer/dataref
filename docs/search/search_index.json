{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Purpose The aim of this document is to serve as the definitive reference for data handling in Break Through Cancer (BTC), establishing the consensus rubric under which data are identified, categorized, generated, aggregated, accessed, and governed. This includes metadata capture during patient enrollment and sample acquisition, standards and processes for molecular assay data generation and pipelines, data flow diagrams providing simplified views, as well as a FAQ for common questions. This is pursued in concert with BTC Disease TeamLabs, and falls within the purview of the larger Data Science TeamLab (DST) . The system and infrastructure which serves as the convergence point for data science activity within BTC is code-named DASH , short for DA ta S cience H ub. The practice of data science in BTC is informed by numerous standards, including F.A.I.R. practice and NIH guidelines , and draws heavily from lessons learned and software developed in earlier and sister projects, including TCGA , GDC , and HTAN . Data Life Cycle The general flow of data within BTC is as follows: It is helpful to distinguish that the BTC data ecosystem has 2 layers of context: the first encompasses institutional and especially clinical trial activity, the other encompasses data activity vis-a-vis DASH proper. Layer 1 : Institutions conduct clinical trials and collect sample data in their own systems, per their current practice. This originating data will have trial-specific IDs & categorizations relevant in the context of that trial or institution. And is where PHI is maintained; such PHI will be removed prior to ingest to BTC. Layer 2: BTC will assign BTC-specific IDs when data are ingested into DASH. The BTC nomenclature and IDs supplement those of the institution and trial, rather than replace. The mapping between BTC and trial IDs will be retained internally by BTC, and is important provenance to prevent orphaning (see below). Staging Area The staging area is an abstraction that serves as the entryway into DASH: staging of data generated within a disease TeamLab signifies that it is ready to be freely shared amongst other members of the team without encumbrance. This can be done in one of two ways: first , to simplify and expedite immediate data sharing by using existing drag-n-drop infrastructure wherever possible, small images, figures and documents (including pre-clinical) can be deposited directly to the Data folder within the team's respective SharePoint area; second , larger data, such as the molecular results of DNA or scRNA sequencing assays, should be uploaded to data lake cloud storage. To aid clarity and automation, we suggest organizing files into consistent subfolder names according to their data type, along the lines of Submitting and Tracking Please contact the DASH team when your TeamLab is ready to add data to DASH. We'll be happy to guide the process and help decide which staging method is appropriate, as well as perform initial data validation. This validation may include PHI assessment, verifying that trial-specific IDs (when applicable) have been entered into the data submission tracker and associated with corresponding BTC identifiers, and that sufficient metadata are captured. Sharing and Orphaning It is important to note that Submitting data to DASH does not confer access to other TeamLabs in BTC: until the TeamLab collectively decides to provide such data to other TeamLabs, access is limited to members of the submitting TeamLab Clinical lab manuals and SOPs outline steps for registering trial-specific subject and sample IDs in DASH, to prevent orphaning : when the provenance of physical material or data cannot be established because it's been shipped from one institute to another either (a) without trial-specific IDs or (b) recording those trial-specific IDs in DASH and assigning corresponding BTC IDs Data Lake After staging, data will be migrated to the data lake: a semi-organized storage area of cloud buckets containing the files \"as deposited,\" meaning no additional processing, subdivision or interpretation has been performed upon the uploaded files--other than stratification by BTC identifiers, TeamLab and assay data type. Cloud buckets in the data lake will be accessible for systematic analysis in downstream pipelines and tools where the medium of exchange is coarse grained--at the level of entire files rather than portions of content within the files. Finer grained access, such as the ability to query individual rows, columns or data elements within the files (e.g. the expression level of a single gene from a given sample) is typically not performed directly from a data lake but rather via downstream database queries, dashboards or visual-exploratory interfaces (further right in the data life cycle). Warehouse The purpose of the data warehouse is to provide additional validation and cataloging of data within the lake, distilled into high-level dashboards and portals queryable by way of a simple, point-click interface. Due to the ease with which they provide comprehensive summary views of data-intensive science, such portals can be an important means of initial engagement with research data. The BTC warehouse is under construction, but many other examples abound, including the GDC and HTAN ; here we show an excerpt from the GTEx portal: Analysis and Pipelines The DST is working with disease teams to compile data and analysis needs cutting across BTC efforts . For each data type in this list the DST will provide one or more pipelines for primary data generation (Levels 1 and 2, i.e. L1, L2) as well as subsequent L3 and L4 analyses. This work is still early but very active, and a number of analysis pipelines are already available from Terra , MDAnderson, NextFlow nf-core and the MIT BioMicroCenter . Please contact the DASH team for more information if you are unsure of how to proceed. BTC Identifier Scheme Subject and sample (biospecimen) identifiers will be attached to BTC data as follows: Here \u201cbiospecimen\u201d is preferred over \u201csample\u201d for generality and to capture that samples can be subdivided into multiple portions. The association between data file and biospecimen is maintained as metadata within the DASH database, not within the file identifier itself. A hypothetical data tree for the first subject (patient) of the BTC glioblastoma multiforme trial might look like Here 6 needle biopsy cores (samples) were extracted; the first of which has been characterized in multiple assays, yielding 8 distinctly molecular data output files (i.e. one per data type). Each interventional timepoint in a longitudinal trial would yield a new sample (or samples) for that subject. Data Entities and Levels As data flow in BTC they are processed and transformed, passing through a series of \u201cdata levels.\u201d For each data context (e.g. clinical, imaging, sequencing, spatial) the constituent files of each data level may differ, but in all cases Level 1 represents raw or uncurated data (e.g. directly from an instrument) and each successive level represents a maturation of that data towards analysis and, eventually, publication and wider utilization. Unless explicitly stated otherwise, we propose BTC data infrastructure, processing and analysis adopt existing GDC + HTAN standards and nomenclature, including for clinical , biospecimen , sequencing and imaging data. For convenience, some of those standards will be excerpted below but we refer the reader back to the original material at the given links for an exhaustive treatment. Each disease project in BTC contributes data from 1 or more enrolled Subjects, who have donated Biospecimens. Data files are generated when biospecimens are assayed by an instrument/protocol or processed in downstream SOPs or analyses. Captured metadata enables tracing of any data file back to its source biospecimen. Level 1 raw data files are derived directly from the corresponding biospecimen, whereas processed level 2-4 data files are linked to lower level parent data files. Clinical Data Levels (Tiers) The BTC clinical data model consists of three tiers. Tier 1 aligns with Genomic Data Commons (GDC) guidelines for clinical data, while Tiers 2 and 3 are informed by the HTAN extensions to the GDC model . Clinical data in BTC are still evolving as we develop trial forms and SOPs across institutions. Only Tier1 is encompassed thus far, but no data will be ingested into DASH if it is not (a) fully de-identified and (b) accompanied by minimally viable metadata (biospecimen and/or clinical). Omic Data Levels In alignment with TCGA and the NCI Genomic Data Commons, BTC will categorize multi-omic data into four levels: Level Definition Examples 1 Raw data FASTQs, unaligned BAMs 2 Aligned primary data Aligned BAMs 3 Derived biomolecular data Gene expression matrix files, VCFs, etc 4 Sample level summary data t-SNE plot coordinates, etc These will apply to the multiple assay and sequencing modalities (omic data types) envisioned for use in BTC shown here, , including single-cell and single-nucleus RNA Seq (sc/snRNASeq), single-cell ATAC Seq, bulk RNAseq and Bulk DNAseq. We propose that BTC follow the latest GENCODE version for gene annotations, GENCODE Version 43. GENCODE is used for gene definitions by many consortia, including ENCODE, NCI Genomic Data Commons, Human Cell Atlas, and PCAWG (Pan-Cancer Analysis of Whole Genomes). Ensembl gene content is essentially identical to that of GENCODE (FAQ) and interconversion is possible. We further propose BTC adopt GENCODE 43 Gene Transfer Format (GTF) basic gene annotation file (GENCODE 43 GTF) and filtered files (GENCODE 43 GTF with genes only; GENCODE 43 GTF with genes only and retaining only chromosome X copy of pseudoautosomal region) for gene annotation. BTC may also include external data generated with other gene models, as the process of implementing the standard is ongoing. Within BTC metadata files, we propose the reference genome in use be specified in columns/attributes named \u201cGenomic Reference\u201d and \u201cGenomic Reference URL\u201d. External Data Data generated through efforts funded by other organizations is referred to as \u201cexternal data\u201d. BTC investigators are free (and encouraged) to utilize external data in BTC work, which will typically play out in one of two ways: Ad-hoc: in which the investigator or their staff downloads external data to local institutional resources (e.g. on-prem compute or cloud) and references in their local BTC analyses; here the investigator and/or staff initiates & performs the collecting, aggregating and storing of external data on their institutional systems DASH: investigator requests that BTC make external easily accessible to other BTC collaborators via DASH; either in raw form directly from the data lake or more formally with BTC identifiers attached so that the identity and provenance are clear In the latter case of external data being assigned BTC IDs, a unique project abbreviation will be created to indicate that the dataset is from an external source, following the schema EXT_ . For example, if we were to load BRCA data from TCGA into DASH, one might use TCGA_BRCA as the project abbreviation, yielding identifiers for such data that begin with BTC-TCGA_BRCA- and so forth. Attaching BTC identifiers to external data may provide a number of advantages Enabling it to be seamlessly mingled with internal BTC data Then processed and analyzed at scale in downstream pipelines or analysis tools While making the external identity and provenance of the data clear And simplifying later bookkeeping and database tracing when assembling data for publication but should not be interpreted as a claim that BTC now \u201cowns\u201d or is attempting to \u201cre-brand\u201d those external data. Version 0.50","title":"  "},{"location":"#purpose","text":"The aim of this document is to serve as the definitive reference for data handling in Break Through Cancer (BTC), establishing the consensus rubric under which data are identified, categorized, generated, aggregated, accessed, and governed. This includes metadata capture during patient enrollment and sample acquisition, standards and processes for molecular assay data generation and pipelines, data flow diagrams providing simplified views, as well as a FAQ for common questions. This is pursued in concert with BTC Disease TeamLabs, and falls within the purview of the larger Data Science TeamLab (DST) . The system and infrastructure which serves as the convergence point for data science activity within BTC is code-named DASH , short for DA ta S cience H ub. The practice of data science in BTC is informed by numerous standards, including F.A.I.R. practice and NIH guidelines , and draws heavily from lessons learned and software developed in earlier and sister projects, including TCGA , GDC , and HTAN .","title":"Purpose"},{"location":"#data-life-cycle","text":"The general flow of data within BTC is as follows: It is helpful to distinguish that the BTC data ecosystem has 2 layers of context: the first encompasses institutional and especially clinical trial activity, the other encompasses data activity vis-a-vis DASH proper. Layer 1 : Institutions conduct clinical trials and collect sample data in their own systems, per their current practice. This originating data will have trial-specific IDs & categorizations relevant in the context of that trial or institution. And is where PHI is maintained; such PHI will be removed prior to ingest to BTC. Layer 2: BTC will assign BTC-specific IDs when data are ingested into DASH. The BTC nomenclature and IDs supplement those of the institution and trial, rather than replace. The mapping between BTC and trial IDs will be retained internally by BTC, and is important provenance to prevent orphaning (see below).","title":"Data Life Cycle"},{"location":"#staging-area","text":"The staging area is an abstraction that serves as the entryway into DASH: staging of data generated within a disease TeamLab signifies that it is ready to be freely shared amongst other members of the team without encumbrance. This can be done in one of two ways: first , to simplify and expedite immediate data sharing by using existing drag-n-drop infrastructure wherever possible, small images, figures and documents (including pre-clinical) can be deposited directly to the Data folder within the team's respective SharePoint area; second , larger data, such as the molecular results of DNA or scRNA sequencing assays, should be uploaded to data lake cloud storage. To aid clarity and automation, we suggest organizing files into consistent subfolder names according to their data type, along the lines of","title":"Staging Area"},{"location":"#submitting-and-tracking","text":"Please contact the DASH team when your TeamLab is ready to add data to DASH. We'll be happy to guide the process and help decide which staging method is appropriate, as well as perform initial data validation. This validation may include PHI assessment, verifying that trial-specific IDs (when applicable) have been entered into the data submission tracker and associated with corresponding BTC identifiers, and that sufficient metadata are captured.","title":"Submitting and Tracking"},{"location":"#sharing-and-orphaning","text":"It is important to note that Submitting data to DASH does not confer access to other TeamLabs in BTC: until the TeamLab collectively decides to provide such data to other TeamLabs, access is limited to members of the submitting TeamLab Clinical lab manuals and SOPs outline steps for registering trial-specific subject and sample IDs in DASH, to prevent orphaning : when the provenance of physical material or data cannot be established because it's been shipped from one institute to another either (a) without trial-specific IDs or (b) recording those trial-specific IDs in DASH and assigning corresponding BTC IDs","title":"Sharing and Orphaning"},{"location":"#data-lake","text":"After staging, data will be migrated to the data lake: a semi-organized storage area of cloud buckets containing the files \"as deposited,\" meaning no additional processing, subdivision or interpretation has been performed upon the uploaded files--other than stratification by BTC identifiers, TeamLab and assay data type. Cloud buckets in the data lake will be accessible for systematic analysis in downstream pipelines and tools where the medium of exchange is coarse grained--at the level of entire files rather than portions of content within the files. Finer grained access, such as the ability to query individual rows, columns or data elements within the files (e.g. the expression level of a single gene from a given sample) is typically not performed directly from a data lake but rather via downstream database queries, dashboards or visual-exploratory interfaces (further right in the data life cycle).","title":"Data Lake"},{"location":"#warehouse","text":"The purpose of the data warehouse is to provide additional validation and cataloging of data within the lake, distilled into high-level dashboards and portals queryable by way of a simple, point-click interface. Due to the ease with which they provide comprehensive summary views of data-intensive science, such portals can be an important means of initial engagement with research data. The BTC warehouse is under construction, but many other examples abound, including the GDC and HTAN ; here we show an excerpt from the GTEx portal:","title":"Warehouse"},{"location":"#analysis-and-pipelines","text":"The DST is working with disease teams to compile data and analysis needs cutting across BTC efforts . For each data type in this list the DST will provide one or more pipelines for primary data generation (Levels 1 and 2, i.e. L1, L2) as well as subsequent L3 and L4 analyses. This work is still early but very active, and a number of analysis pipelines are already available from Terra , MDAnderson, NextFlow nf-core and the MIT BioMicroCenter . Please contact the DASH team for more information if you are unsure of how to proceed.","title":"Analysis and Pipelines"},{"location":"#btc-identifier-scheme","text":"Subject and sample (biospecimen) identifiers will be attached to BTC data as follows: Here \u201cbiospecimen\u201d is preferred over \u201csample\u201d for generality and to capture that samples can be subdivided into multiple portions. The association between data file and biospecimen is maintained as metadata within the DASH database, not within the file identifier itself. A hypothetical data tree for the first subject (patient) of the BTC glioblastoma multiforme trial might look like Here 6 needle biopsy cores (samples) were extracted; the first of which has been characterized in multiple assays, yielding 8 distinctly molecular data output files (i.e. one per data type). Each interventional timepoint in a longitudinal trial would yield a new sample (or samples) for that subject.","title":"BTC Identifier Scheme"},{"location":"#data-entities-and-levels","text":"As data flow in BTC they are processed and transformed, passing through a series of \u201cdata levels.\u201d For each data context (e.g. clinical, imaging, sequencing, spatial) the constituent files of each data level may differ, but in all cases Level 1 represents raw or uncurated data (e.g. directly from an instrument) and each successive level represents a maturation of that data towards analysis and, eventually, publication and wider utilization. Unless explicitly stated otherwise, we propose BTC data infrastructure, processing and analysis adopt existing GDC + HTAN standards and nomenclature, including for clinical , biospecimen , sequencing and imaging data. For convenience, some of those standards will be excerpted below but we refer the reader back to the original material at the given links for an exhaustive treatment. Each disease project in BTC contributes data from 1 or more enrolled Subjects, who have donated Biospecimens. Data files are generated when biospecimens are assayed by an instrument/protocol or processed in downstream SOPs or analyses. Captured metadata enables tracing of any data file back to its source biospecimen. Level 1 raw data files are derived directly from the corresponding biospecimen, whereas processed level 2-4 data files are linked to lower level parent data files.","title":"Data Entities and Levels"},{"location":"#clinical-data-levels-tiers","text":"The BTC clinical data model consists of three tiers. Tier 1 aligns with Genomic Data Commons (GDC) guidelines for clinical data, while Tiers 2 and 3 are informed by the HTAN extensions to the GDC model . Clinical data in BTC are still evolving as we develop trial forms and SOPs across institutions. Only Tier1 is encompassed thus far, but no data will be ingested into DASH if it is not (a) fully de-identified and (b) accompanied by minimally viable metadata (biospecimen and/or clinical).","title":"Clinical Data Levels (Tiers)"},{"location":"#omic-data-levels","text":"In alignment with TCGA and the NCI Genomic Data Commons, BTC will categorize multi-omic data into four levels: Level Definition Examples 1 Raw data FASTQs, unaligned BAMs 2 Aligned primary data Aligned BAMs 3 Derived biomolecular data Gene expression matrix files, VCFs, etc 4 Sample level summary data t-SNE plot coordinates, etc These will apply to the multiple assay and sequencing modalities (omic data types) envisioned for use in BTC shown here, , including single-cell and single-nucleus RNA Seq (sc/snRNASeq), single-cell ATAC Seq, bulk RNAseq and Bulk DNAseq. We propose that BTC follow the latest GENCODE version for gene annotations, GENCODE Version 43. GENCODE is used for gene definitions by many consortia, including ENCODE, NCI Genomic Data Commons, Human Cell Atlas, and PCAWG (Pan-Cancer Analysis of Whole Genomes). Ensembl gene content is essentially identical to that of GENCODE (FAQ) and interconversion is possible. We further propose BTC adopt GENCODE 43 Gene Transfer Format (GTF) basic gene annotation file (GENCODE 43 GTF) and filtered files (GENCODE 43 GTF with genes only; GENCODE 43 GTF with genes only and retaining only chromosome X copy of pseudoautosomal region) for gene annotation. BTC may also include external data generated with other gene models, as the process of implementing the standard is ongoing. Within BTC metadata files, we propose the reference genome in use be specified in columns/attributes named \u201cGenomic Reference\u201d and \u201cGenomic Reference URL\u201d.","title":"Omic Data Levels"},{"location":"#external-data","text":"Data generated through efforts funded by other organizations is referred to as \u201cexternal data\u201d. BTC investigators are free (and encouraged) to utilize external data in BTC work, which will typically play out in one of two ways: Ad-hoc: in which the investigator or their staff downloads external data to local institutional resources (e.g. on-prem compute or cloud) and references in their local BTC analyses; here the investigator and/or staff initiates & performs the collecting, aggregating and storing of external data on their institutional systems DASH: investigator requests that BTC make external easily accessible to other BTC collaborators via DASH; either in raw form directly from the data lake or more formally with BTC identifiers attached so that the identity and provenance are clear In the latter case of external data being assigned BTC IDs, a unique project abbreviation will be created to indicate that the dataset is from an external source, following the schema EXT_ . For example, if we were to load BRCA data from TCGA into DASH, one might use TCGA_BRCA as the project abbreviation, yielding identifiers for such data that begin with BTC-TCGA_BRCA- and so forth. Attaching BTC identifiers to external data may provide a number of advantages Enabling it to be seamlessly mingled with internal BTC data Then processed and analyzed at scale in downstream pipelines or analysis tools While making the external identity and provenance of the data clear And simplifying later bookkeeping and database tracing when assembling data for publication but should not be interpreted as a claim that BTC now \u201cowns\u201d or is attempting to \u201cre-brand\u201d those external data.","title":"External Data"},{"location":"#version-050","text":"","title":"Version 0.50"},{"location":"faq/","text":"BTC Data Science Frequently Asked Questions .bs-sidebar { display: none; } Who is responsible for data analysis within BTC disease team labs? There is fuzzy overlap in responsibility for analysis: it should be driven to the greatest extent possible by the disease teamlab, as reflected in their respective budgets, and supported by the Data Science TeamLab (DST) in whatever means necessary (whether computational analytics, data engineering, visualization, etc), but not wholly outsourced from the disease teamlab to the DST. Where that line is drawn for each scientific question or project will likely shift\u2014and may also depend upon trial timelines and staffing\u2014-probably in some cases the data scientists may need to pick up more of the analysis, in others less so. But the overall view, at least initially while BTC finds its operational footing vis-\u00e0-vis data science, is that the DST is not \u201cjust another core, service-oriented facility,\u201d but rather is a resource of expert collaborators who devise new methods (when needed), or assist with data gathering/engineering, or provide analytic insight and/or assist with interpretation. What pipelines will be available to assist with my analysis? This is discussed in the data reference . What assays (data types) will the pipelines operate upon? This is also discussed in the data reference . Ideally, what kind of biospecimen metadata will be collected? Sample acquisition method, e.g. autopsy, biopsy, fine needle aspirate, etc. Topography Code, indicating site within the body, e.g. based on ICD-O-3. Collection information e.g. time, duration of ischemia, temperature, etc. Processing of parent biospecimen information e.g. fresh, frozen, etc. Biospecimen and derivative clinical metadata I.e. Histologic Morphology Code, e.g. based on ICD-O-3. Coordinates for derivative biospecimen from their parent biospecimen. Processing of derivative biospecimen for downstream analysis e.g. dissociation, sectioning, analyte isolation This list is adapted from HTAN standards and is not intended to be mandatory constraints imposed by the DST upon clinical trial teams or disease team labs; we are in the process of defining the minimum viable set of clinical data elements (CDEs) for BTC projects. How do I submit data from my TeamLab into DASH? This is discussed in the data reference .","title":"FAQ"},{"location":"faq/#btc-data-science-frequently-asked-questions","text":".bs-sidebar { display: none; }","title":"BTC Data Science Frequently Asked Questions"},{"location":"faq/#_1","text":"Who is responsible for data analysis within BTC disease team labs? There is fuzzy overlap in responsibility for analysis: it should be driven to the greatest extent possible by the disease teamlab, as reflected in their respective budgets, and supported by the Data Science TeamLab (DST) in whatever means necessary (whether computational analytics, data engineering, visualization, etc), but not wholly outsourced from the disease teamlab to the DST. Where that line is drawn for each scientific question or project will likely shift\u2014and may also depend upon trial timelines and staffing\u2014-probably in some cases the data scientists may need to pick up more of the analysis, in others less so. But the overall view, at least initially while BTC finds its operational footing vis-\u00e0-vis data science, is that the DST is not \u201cjust another core, service-oriented facility,\u201d but rather is a resource of expert collaborators who devise new methods (when needed), or assist with data gathering/engineering, or provide analytic insight and/or assist with interpretation. What pipelines will be available to assist with my analysis? This is discussed in the data reference . What assays (data types) will the pipelines operate upon? This is also discussed in the data reference . Ideally, what kind of biospecimen metadata will be collected? Sample acquisition method, e.g. autopsy, biopsy, fine needle aspirate, etc. Topography Code, indicating site within the body, e.g. based on ICD-O-3. Collection information e.g. time, duration of ischemia, temperature, etc. Processing of parent biospecimen information e.g. fresh, frozen, etc. Biospecimen and derivative clinical metadata I.e. Histologic Morphology Code, e.g. based on ICD-O-3. Coordinates for derivative biospecimen from their parent biospecimen. Processing of derivative biospecimen for downstream analysis e.g. dissociation, sectioning, analyte isolation This list is adapted from HTAN standards and is not intended to be mandatory constraints imposed by the DST upon clinical trial teams or disease team labs; we are in the process of defining the minimum viable set of clinical data elements (CDEs) for BTC projects. How do I submit data from my TeamLab into DASH? This is discussed in the data reference .","title":""}]}